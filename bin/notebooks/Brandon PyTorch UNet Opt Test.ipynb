{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script takes work from Pytorch MNIST and UNet Dev.ipynb, takes the UNet portion only, and makes \n",
    "# training and validation repeatable in order\n",
    "# to test a method for saving and setting optimizer state (momentum, etc.) that remains consistent\n",
    "# even when the restoring is done after restarting the kernel. The train_epoch method also has been reduced to training\n",
    "# over a few batches (not a complete epoch) in order to make testing faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class FLModel(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_tensor_dict(self):\n",
    "        \"\"\"Returns all parameters for aggregation, including optimizer parameters, if appropriate\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def set_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"Returns all parameters for aggregation, including optimizer parameters, if appropriate\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train_partial_epoch(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_training_data_size(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def validate(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_validation_data_size(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import socket\n",
    "\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def _get_dataset_func_map():\n",
    "    return {\n",
    "        'mnist': load_mnist,\n",
    "#         'fashion-mnist': load_fashion_mnist,\n",
    "#         'pubfig83': load_pubfig83,\n",
    "#         'cifar10': load_cifar10,\n",
    "#         'cifar20': load_cifar20,\n",
    "#         'cifar100': load_cifar100,\n",
    "#         'bsm': load_bsm,\n",
    "#         'BraTS17': load_BraTS17,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_dataset_list():\n",
    "    return list(_get_dataset_func_map().keys())\n",
    "\n",
    "\n",
    "def load_dataset(dataset, **kwargs):\n",
    "    if dataset not in get_dataset_list():\n",
    "        raise ValueError(\"Dataset {} not in list of datasets {get_dataset_list()}\".format(dataset))\n",
    "    return _get_dataset_func_map()[dataset](**kwargs)\n",
    "\n",
    "\n",
    "def _get_dataset_dir(server='edwardsb-Z270x-UD5'):\n",
    "    if server is None:\n",
    "        server = socket.gethostname()\n",
    "    server_to_path = {'spr-gpu01': os.path.join('/', 'raid', 'datasets'),\n",
    "                      'edwardsb-Z270X-UD5': os.path.join('/', 'data'),\n",
    "                      'msheller-ubuntu': os.path.join('/', 'home', 'msheller', 'datasets')}\n",
    "    return server_to_path[server]\n",
    "\n",
    "\n",
    "def _unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='bytes')\n",
    "    return d\n",
    "\n",
    "\n",
    "def _read_mnist(path, **kwargs):\n",
    "    X_train, y_train = _read_mnist_kind(path, kind='train', **kwargs)\n",
    "    X_test, y_test = _read_mnist_kind(path, kind='t10k', **kwargs)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# from https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "def _read_mnist_kind(path, kind='train', one_hot=True, **kwargs):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    images = images.astype(float) / 255\n",
    "    if one_hot:\n",
    "        labels = _one_hot(labels.astype(np.int), 10)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_mnist(**kwargs):\n",
    "    path = os.path.join(_get_dataset_dir(), 'mnist', 'input_data')\n",
    "    return _read_mnist(path, **kwargs)\n",
    "\n",
    "\n",
    "def load_fashion_mnist(**kwargs):\n",
    "    path = os.path.join(_get_dataset_dir(), 'fashion-mnist')\n",
    "    return _read_mnist(path, **kwargs)\n",
    "\n",
    "\n",
    "def _one_hot(y, n):\n",
    "    return np.eye(n)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PyTorchFLModel(FLModel, nn.Module):\n",
    "    \"\"\"WIP code. Goal is to simplify porting a model to this framework.\n",
    "    Currently, this creates a placeholder and assign op for every variable, which grows the graph considerably.\n",
    "    Also, the abstraction for the tf.session isn't ideal yet. optimizer work is modeified by Brandon\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # calls nn.Module init\n",
    "        super(PyTorchFLModel, self).__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_optimizer(self):\n",
    "        pass\n",
    "\n",
    "    def get_optimizer_tensors(self):\n",
    "        optimizer = self.get_optimizer()\n",
    "\n",
    "        tensor_dict = {}\n",
    "\n",
    "        # FIXME: not clear that this works consistently across optimizers\n",
    "        # FIXME: hard-coded naming convention sucks and could absolutely break\n",
    "        i = 0\n",
    "        for group in optimizer.state_dict()['param_groups']:\n",
    "            for key in group['params']:\n",
    "                tensor_dict['__opt_{}'.format(i)] = \\\n",
    "                  optimizer.state_dict()['state'][key]['momentum_buffer'].detach().cpu().numpy()\n",
    "                i += 1\n",
    "        return tensor_dict\n",
    "            \n",
    "    def set_optimizer_tensors(self, tensor_dict):\n",
    "        optimizer = self.get_optimizer()\n",
    "        \n",
    "        # Relies on consistent ordering of the keys obtained through optimizer.state_dict()['param_groups'][i]['params']\n",
    "        # for each i, as well as the consistent ordering of these groups.\n",
    "        # FIXME: not clear that this works consistently across optimizers\n",
    "        # FIXME: hard-coded naming convention sucks and could absolutely break?\n",
    "        i = 0\n",
    "        for group in optimizer.state_dict()['param_groups']:\n",
    "            for key in group['params']:\n",
    "                tensor = optimizer.state_dict()['state'][key]['momentum_buffer']\n",
    "                # print(\"Before value: {}\".format(optimizer.state_dict()['state'][key]['momentum_buffer']))\n",
    "                optimizer.state_dict()['state'][key]['momentum_buffer'] = torch.Tensor(tensor_dict['__opt_{}'.format(i)]).to(tensor.device)\n",
    "                # print(\"After value: {}\".format(optimizer.state_dict()['state'][key]['momentum_buffer']))\n",
    "                i += 1\n",
    "        \n",
    "    def get_tensor_dict(self):\n",
    "        # FIXME: should we use self.parameters()??? Unclear if load_state_dict() is better or simple assignment is better\n",
    "        # for now, state dict gives us names, which is good\n",
    "\n",
    "        # FIXME: do both and sanity check each time?\n",
    "\n",
    "        # FIXME: can this have values other than the tensors????\n",
    "        state = self.state_dict()\n",
    "        for k, v in state.items():\n",
    "            state[k] = v.cpu().numpy() # get as a numpy array\n",
    "        return {**state, **self.get_optimizer_tensors()}\n",
    "\n",
    "    def set_tensor_dict(self, tensor_dict):\n",
    "        # FIXME: should we use self.parameters()??? Unclear if load_state_dict() is better or simple assignment is better\n",
    "        # for now, state dict gives us names, which is good\n",
    "        \n",
    "        # FIXME: do both and sanity check each time?\n",
    "\n",
    "        # get the model state so that we can determine the correct tensor values/device placements\n",
    "        model_state = self.state_dict()\n",
    "\n",
    "        new_state = {}\n",
    "        for k, v in model_state.items():\n",
    "            new_state[k] = torch.Tensor(tensor_dict[k]).to(v.device)\n",
    "\n",
    "        # set model state\n",
    "        self.load_state_dict(new_state)\n",
    "\n",
    "        # next we have the optimizer state\n",
    "        self.set_optimizer_tensors(tensor_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tfedlrn.datasets import load_dataset\n",
    "# from tfedlrn.collaborator.pytorchflmodel import PyTorchFLModel\n",
    "\n",
    "\n",
    "def dice_coef(pred, target, smoothing=1.0):    \n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = (pred + target).sum(dim=(1, 2, 3))\n",
    "    \n",
    "    return ((2 * intersection + smoothing) / (union + smoothing)).mean()\n",
    "\n",
    "\n",
    "def dice_coef_loss(pred, target, smoothing=1.0):    \n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = (pred + target).sum(dim=(1, 2, 3))\n",
    "    \n",
    "    term1 = -torch.log(2 * intersection + smoothing)\n",
    "    term2 = torch.log(union + smoothing)\n",
    "    \n",
    "    return term1.mean() + term2.mean()\n",
    "\n",
    "\n",
    "class PyTorch2DUNet(PyTorchFLModel):\n",
    "\n",
    "    def __init__(self, device, train_loader=None, val_loader=None, optimizer='SGD'):\n",
    "        super(PyTorch2DUNet, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.init_data_pipeline(train_loader, val_loader)\n",
    "        self.init_network(device)\n",
    "        self.init_optimizer(optimizer)\n",
    "        \n",
    "    def create_loader(self, X, y, **kwargs):\n",
    "        tX = torch.stack([torch.Tensor(i) for i in X])\n",
    "        ty = torch.stack([torch.Tensor(i) for i in y])\n",
    "        return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(tX, ty), **kwargs)\n",
    "\n",
    "    # FIXME: brats loading\n",
    "    def init_data_pipeline(self, train_loader, val_loader):\n",
    "        if train_loader is None or val_loader is None:\n",
    "            # load all the institutions\n",
    "            data_by_institution = [load_dataset('BraTS17_institution',\n",
    "                                                channels_first=True,\n",
    "                                                institution=i) for i in range(10)]\n",
    "            data_by_type = zip(*data_by_institution)\n",
    "            data_by_type = [np.concatenate(d) for d in data_by_type]\n",
    "            X_train, y_train, X_val, y_val = data_by_type\n",
    "\n",
    "        #TODO: Replace both shuffle=False below with shuffle=True, currently testing and so want reproducibility\n",
    "        if train_loader is None:\n",
    "            self.train_loader = self.create_loader(X_train, y_train, batch_size=64, shuffle=False)\n",
    "        else:\n",
    "            self.train_loader = train_loader\n",
    "\n",
    "        if val_loader is None:\n",
    "            self.val_loader = self.create_loader(X_val, y_val, batch_size=64, shuffle=False)\n",
    "        else:\n",
    "            self.val_loader = val_loader\n",
    "            \n",
    "    def init_network(self,\n",
    "                     device,\n",
    "                     initial_channels=1,\n",
    "                     depth_per_side=5,\n",
    "                     initial_filters=32):\n",
    "\n",
    "        f = initial_filters\n",
    "        \n",
    "        # store our depth for our forward function\n",
    "        self.depth_per_side = 5\n",
    "        \n",
    "        # parameter-less layers\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "                \n",
    "        # initial down layers\n",
    "        conv_down_a = [nn.Conv2d(initial_channels, f, 3, padding=1)]\n",
    "        conv_down_b = [nn.Conv2d(f, f, 3, padding=1)]\n",
    "                \n",
    "        # rest of the layers going down\n",
    "        for i in range(1, depth_per_side):\n",
    "            f *= 2\n",
    "            conv_down_a.append(nn.Conv2d(f // 2, f, 3, padding=1))\n",
    "            conv_down_b.append(nn.Conv2d(f, f, 3, padding=1))\n",
    "            \n",
    "        # going up, do all but the last layer\n",
    "        conv_up_a = []\n",
    "        conv_up_b = []\n",
    "        for _ in range(depth_per_side-1):\n",
    "            f //= 2\n",
    "            # triple input channels due to skip connections\n",
    "            conv_up_a.append(nn.Conv2d(f*3, f, 3, padding=1))\n",
    "            conv_up_b.append(nn.Conv2d(f, f, 3, padding=1))\n",
    "            \n",
    "        # do the last layer\n",
    "        self.conv_out = nn.Conv2d(f, 1, 1, padding=0)\n",
    "        \n",
    "        # all up/down layers need to to become fields of this object\n",
    "        for i, (a, b) in enumerate(zip(conv_down_a, conv_down_b)):\n",
    "            setattr(self, 'conv_down_{}a'.format(i+1), a)\n",
    "            setattr(self, 'conv_down_{}b'.format(i+1), b)\n",
    "            \n",
    "        # all up/down layers need to to become fields of this object\n",
    "        for i, (a, b) in enumerate(zip(conv_up_a, conv_up_b)):\n",
    "            setattr(self, 'conv_up_{}a'.format(i+1), a)\n",
    "            setattr(self, 'conv_up_{}b'.format(i+1), b)\n",
    "        \n",
    "        # send this to the device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # gather up our up and down layer members for easier processing\n",
    "        conv_down_a = [getattr(self, 'conv_down_{}a'.format(i+1)) for i in range(self.depth_per_side)]\n",
    "        conv_down_b = [getattr(self, 'conv_down_{}b'.format(i+1)) for i in range(self.depth_per_side)]\n",
    "        conv_up_a = [getattr(self, 'conv_up_{}a'.format(i+1)) for i in range(self.depth_per_side - 1)]\n",
    "        conv_up_b = [getattr(self, 'conv_up_{}b'.format(i+1)) for i in range(self.depth_per_side - 1)]\n",
    "        \n",
    "        # we concatenate the outputs from the b layers\n",
    "        concat_me = []\n",
    "        pool = x\n",
    "\n",
    "        # going down, wire each pair and then pool except the last\n",
    "        for a, b in zip(conv_down_a, conv_down_b):\n",
    "            out_down = F.relu(b(F.relu(a(pool))))\n",
    "            # if not the last down b layer, pool it and add it to the concat list\n",
    "            if b != conv_down_b[-1]:\n",
    "                concat_me.append(out_down)\n",
    "                pool = self.maxpool(out_down) # feed the pool into the next layer\n",
    "        \n",
    "        # reverse the concat_me layers\n",
    "        concat_me = concat_me[::-1]\n",
    "\n",
    "        # we start going up with the b (not-pooled) from previous layer\n",
    "        in_up = out_down\n",
    "\n",
    "        # going up, we need to zip a, b and concat_me\n",
    "        for a, b, c in zip(conv_up_a, conv_up_b, concat_me):\n",
    "            up = torch.cat([self.upsample(in_up), c], dim=1)\n",
    "            in_up = F.relu(b(F.relu(a(up))))\n",
    "        \n",
    "        # finally, return the output\n",
    "        return torch.sigmoid(self.conv_out(in_up))\n",
    "\n",
    "    def init_optimizer(self, optimizer='SGD'):\n",
    "        if optimizer == 'SGD':\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=1e-3, momentum=0.9)\n",
    "        elif optimizer == 'RMSprop':\n",
    "            self.optimizer = optim.RMSprop(self.parameters(), lr=1e-5, momentum=0.9)\n",
    "        elif optimizer == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=1e-5)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def train_partial_epoch(self):\n",
    "        # set to \"training\" mode\n",
    "        self.train()\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            \n",
    "            # TODO: Remove below - storing cpu data to inspect below\n",
    "            cpu_data, cpu_target = data, target\n",
    "            \n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self(data)\n",
    "            loss = dice_coef_loss(output, target, smoothing=32.0)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            #TODO: Remove below- just performing one batch of training now for testing below\n",
    "            info = \"Sum of batch is data: {}, target: {}\".format(np.sum(cpu_data.numpy(), axis=0), \n",
    "                                                                 np.sum(cpu_target.numpy(), axis=0))\n",
    "            break\n",
    "            \n",
    "        return np.mean(losses), info\n",
    "\n",
    "    def get_training_data_size(self):\n",
    "        return len(self.train_loader.dataset)\n",
    "\n",
    "    def validate(self):\n",
    "        self.eval()\n",
    "        dice = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            counter = 0\n",
    "            for data, target in self.val_loader:\n",
    "                samples = target.shape[0]\n",
    "                total_samples += samples\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self(data)\n",
    "                dice += dice_coef(output, target).cpu().numpy() * samples\n",
    "                counter += 1\n",
    "                if counter == 2:\n",
    "                    break\n",
    "        return dice / total_samples\n",
    "\n",
    "    def get_validation_data_size(self):\n",
    "        return len(self.val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### EXPLORING OPTIMIZER PARAMETERS (SAVING AND RESTORING over processes) ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = PyTorch2DUNet(device=device, optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing that the data used for training is reproducible by looking at its sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.55046,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5501776,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing that validation is reproducible for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433784192020539"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433784192020539"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# testing restoring model and optimizer, should result in training to the same validation ##\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433784192020539"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_weights = cnn.get_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for some more, then see that model is different now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5496416,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433564520673826"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see valiation is not the same now\n",
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore saved model\n",
    "cnn.set_tensor_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433784192020539"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see validation is back to previously observed for saved model\n",
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if grabbing the full state again, it gives the same thing as we had saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_2 = cnn.get_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_array = np.array([])\n",
    "for key in model_weights:\n",
    "    np.append(bool_array, [np.all(model_weights[key] == model_weights_2[key])])\n",
    "np.all(bool_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train again and see you get back to the place you did before after running 'train_partial_epoch' once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5496416,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433564520673826"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----now test that restoring can happen across processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('saved_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model_weights, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RESTART KERNEL HERE  then run the top cells of this workbook up to device intialization ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Then return to this point and run cells below ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate model and train a bit, then restore model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = PyTorch2DUNet(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.4738846,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034504899696912616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see valiation is not the same as the ones before\n",
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model weights from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model.pkl', 'rb') as file:\n",
    "    model_weights = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore model to saved values\n",
    "cnn.set_tensor_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433784192020539"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see validation is back to previously observed for saved model\n",
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if grabbing the full state again, it gives the same thing as we had saved before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_2 = cnn.get_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_array = np.array([])\n",
    "for key in model_weights:\n",
    "    np.append(bool_array, [np.all(model_weights[key] == model_weights_2[key])])\n",
    "np.all(bool_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5496416,\n",
       " 'Sum of batch is data: [[[-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  ...\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]\\n  [-50.2968 -50.2968 -50.2968 ... -50.2968 -50.2968 -50.2968]]], target: [[[0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  ...\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]\\n  [0. 0. 0. ... 0. 0. 0.]]]')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now train to get a new validation, and see that it matches what we had after training once before\n",
    "cnn.train_partial_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03433564520673826"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cnn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Solution needs to be more general to acommodate different optimizers #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### For example, the keys for Adam are different ('momentum_buffer' throws a key error) #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfedlrn-CPfKUQY7",
   "language": "python",
   "name": "tfedlrn-cpfkuqy7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
