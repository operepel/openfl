{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class in_conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size = 3, \n",
    "                 dropout_p=0.3, leakiness=1e-2, conv_bias=True, \n",
    "                 inst_norm_affine=True, res = False, lrelu_inplace=True):\n",
    "        \"\"\"[The initial convolution to enter the network, kind of like encode]\n",
    "        \n",
    "        [This function will create the input convolution]\n",
    "        \n",
    "        Arguments:\n",
    "            input_channels {[int]} -- [the input number of channels, in our case\n",
    "                                       the number of modalities]\n",
    "            output_channels {[int]} -- [the output number of channels, will det-\n",
    "                                        -ermine the upcoming channels]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            kernel_size {number} -- [size of filter] (default: {3})\n",
    "            dropout_p {number} -- [dropout probablity] (default: {0.3})\n",
    "            leakiness {number} -- [the negative leakiness] (default: {1e-2})\n",
    "            conv_bias {bool} -- [to use the bias in filters] (default: {True})\n",
    "            inst_norm_affine {bool} -- [affine use in norm] (default: {True})\n",
    "            res {bool} -- [to use residual connections] (default: {False})\n",
    "            lrelu_inplace {bool} -- [To update conv outputs with lrelu outputs] \n",
    "                                    (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.residual = res\n",
    "        self.dropout_p = dropout_p\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.dropout = nn.Dropout3d(dropout_p)  \n",
    "        self.in_0 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_1 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv1 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv2 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function for initial convolution\n",
    "        \n",
    "        [input --> conv0 --> | --> in --> lrelu --> conv1 --> dropout --> in -|\n",
    "                             |                                                |\n",
    "                  output <-- + <-------------------------- conv2 <-- lrelu <--|]\n",
    "        \n",
    "        Arguments:\n",
    "            x {[Tensor]} -- [Takes in a type of torch Tensor]\n",
    "        \n",
    "        Returns:\n",
    "            [Tensor] -- [Returns a torch Tensor]\n",
    "        \"\"\"\n",
    "        x = self.conv0(x)\n",
    "        if self.residual == True:\n",
    "            skip = x\n",
    "        x = F.leaky_relu(self.in_0(x), negative_slope=self.leakiness, \n",
    "                         inplace=self.lrelu_inplace)\n",
    "        x = self.conv1(x)\n",
    "        if self.dropout_p is not None and self.dropout_p > 0:\n",
    "            x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.in_1(x), negative_slope=self.leakiness, \n",
    "                         inplace=self.lrelu_inplace)\n",
    "        x = self.conv2(x)\n",
    "        if self.residual == True:\n",
    "            x = x + skip\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class DownsamplingModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, leakiness=1e-2, \n",
    "                 dropout_p=0.3, kernel_size=3, conv_bias=True, \n",
    "                 inst_norm_affine=True, lrelu_inplace=True):\n",
    "        \"\"\"[To Downsample a given input with convolution operation]\n",
    "        \n",
    "        [This one will be used to downsample a given comvolution while doubling \n",
    "        the number filters]\n",
    "        \n",
    "        Arguments:\n",
    "            input_channels {[int]} -- [The input number of channels are taken\n",
    "                                       and then are downsampled to double usually]\n",
    "            output_channels {[int]} -- [the output number of channels are \n",
    "                                        usually the double of what of input]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            leakiness {float} -- [the negative leakiness] (default: {1e-2})\n",
    "            conv_bias {bool} -- [to use the bias in filters] (default: {True})\n",
    "            inst_norm_affine {bool} -- [affine use in norm] (default: {True})\n",
    "            lrelu_inplace {bool} -- [To update conv outputs with lrelu outputs] \n",
    "                                    (default: {True})\n",
    "        \"\"\"\n",
    "        #nn.Module.__init__(self)\n",
    "        super(DownsamplingModule, self).__init__()\n",
    "        self.dropout_p=dropout_p\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.lrelu_inplace = True\n",
    "        self.in_0 = nn.InstanceNorm3d(output_channels, \n",
    "                                    affine=self.inst_norm_affine,\n",
    "                                    track_running_stats=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, output_channels, kernel_size = 3,\n",
    "                               stride=2, padding=(kernel_size - 1) // 2, \n",
    "                               bias = self.conv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"[This is a forward function for ]\n",
    "        \n",
    "        [input -- > in --> lrelu --> ConvDS --> output]\n",
    "        \n",
    "        Arguments:\n",
    "            x {[Tensor]} -- [Takes in a type of torch Tensor]\n",
    "        \n",
    "        Returns:\n",
    "            [Tensor] -- [Returns a torch Tensor]\n",
    "        \"\"\"\n",
    "        x = F.leaky_relu(self.in_0(self.conv0(x)), \n",
    "                         negative_slope=self.leakiness, \n",
    "                         inplace=self.lrelu_inplace)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class EncodingModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size = 3, \n",
    "                 dropout_p=0.3, leakiness=1e-2, conv_bias=True, \n",
    "                 inst_norm_affine=True, res = False, lrelu_inplace=True):\n",
    "        \"\"\"[The Encoding convolution module to learn the information and use later]\n",
    "            \n",
    "            [This function will create the Learning convolutions]\n",
    "            \n",
    "            Arguments:\n",
    "                input_channels {[int]} -- [the input number of channels, in our case\n",
    "                                           the number of channels from downsample]\n",
    "                output_channels {[int]} -- [the output number of channels, will det-\n",
    "                                            -ermine the upcoming channels]\n",
    "            \n",
    "            Keyword Arguments:\n",
    "                kernel_size {number} -- [size of filter] (default: {3})\n",
    "                dropout_p {number} -- [dropout probablity] (default: {0.3})\n",
    "                leakiness {number} -- [the negative leakiness] (default: {1e-2})\n",
    "                conv_bias {bool} -- [to use the bias in filters] (default: {True})\n",
    "                inst_norm_affine {bool} -- [affine use in norm] (default: {True})\n",
    "                res {bool} -- [to use residual connections] (default: {False})\n",
    "                lrelu_inplace {bool} -- [To update conv outputs with lrelu outputs] \n",
    "                                        (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.res = res\n",
    "        self.dropout_p = dropout_p\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.dropout = nn.Dropout3d(dropout_p)\n",
    "        self.in_0 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_1 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.conv0 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv1 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function for initial convolution\n",
    "        \n",
    "        [input --> | --> in --> lrelu --> conv0 --> dropout --> in -|\n",
    "                   |                                                |\n",
    "        output <-- + <-------------------------- conv1 <-- lrelu <--|]\n",
    "        \n",
    "        Arguments:\n",
    "            x {[Tensor]} -- [Takes in a type of torch Tensor]\n",
    "        \n",
    "        Returns:\n",
    "            [Tensor] -- [Returns a torch Tensor]\n",
    "        \"\"\"\n",
    "        if self.res == True:\n",
    "            skip = x\n",
    "        x = F.leaky_relu(self.in_0(x), negative_slope=self.leakiness, \n",
    "                         inplace=self.lrelu_inplace)\n",
    "        x = self.conv0(x)\n",
    "        if self.dropout_p is not None and self.dropout_p > 0:\n",
    "            x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.in_1(x), negative_slope=self.leakiness, inplace=self.lrelu_inplace)\n",
    "        x = self.conv1(x)\n",
    "        if self.res == True:\n",
    "            x = x + skip\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, size=None, scale_factor=None, mode='nearest', \n",
    "                 align_corners=True):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.align_corners = align_corners\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, \n",
    "                             mode=self.mode, align_corners=self.align_corners)\n",
    "    \n",
    "class UpsamplingModule(nn.Module): \n",
    "    def __init__(self, input_channels, output_channels, leakiness=1e-2, \n",
    "        lrelu_inplace=True, kernel_size=3, scale_factor=2,\n",
    "        conv_bias=True, inst_norm_affine=True):\n",
    "        \"\"\"[summary]\n",
    "        \n",
    "        [description]\n",
    "        \n",
    "        Arguments:\n",
    "            input__channels {[type]} -- [description]\n",
    "            output_channels {[type]} -- [description]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            leakiness {number} -- [description] (default: {1e-2})\n",
    "            lrelu_inplace {bool} -- [description] (default: {True})\n",
    "            kernel_size {number} -- [description] (default: {3})\n",
    "            scale_factor {number} -- [description] (default: {2})\n",
    "            conv_bias {bool} -- [description] (default: {True})\n",
    "            inst_norm_affine {bool} -- [description] (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.scale_factor = scale_factor\n",
    "        self.interpolate = Interpolate(scale_factor=self.scale_factor, mode='trilinear', \n",
    "                                       align_corners=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, output_channels, kernel_size=1,\n",
    "                                stride=1, padding=0, \n",
    "                                bias = self.conv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"[summary]\n",
    "        \n",
    "        [description]\n",
    "        \n",
    "        Extends:\n",
    "        \"\"\"\n",
    "        x = self.conv0(self.interpolate(x))\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class FCNUpsamplingModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, leakiness=1e-2, \n",
    "        lrelu_inplace=True, kernel_size=3, scale_factor=2,\n",
    "        conv_bias=True, inst_norm_affine=True):\n",
    "        \"\"\"[summary]\n",
    "        \n",
    "        [description]\n",
    "        \n",
    "        Arguments:\n",
    "            input__channels {[type]} -- [description]\n",
    "            output_channels {[type]} -- [description]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            leakiness {number} -- [description] (default: {1e-2})\n",
    "            lrelu_inplace {bool} -- [description] (default: {True})\n",
    "            kernel_size {number} -- [description] (default: {3})\n",
    "            scale_factor {number} -- [description] (default: {2})\n",
    "            conv_bias {bool} -- [description] (default: {True})\n",
    "            inst_norm_affine {bool} -- [description] (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.scale_factor = scale_factor\n",
    "        self.interpolate = Interpolate(scale_factor=2**(self.scale_factor-1), mode='trilinear', \n",
    "                                       align_corners=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, output_channels, kernel_size=1,\n",
    "                                stride=1, padding=0, \n",
    "                                bias = self.conv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"[summary]\n",
    "        \n",
    "        [description]\n",
    "        \n",
    "        Extends:\n",
    "        \"\"\"\n",
    "        #print(\"Pre interpolate and conv:\", x.shape)\n",
    "        x = self.interpolate(self.conv0(x))\n",
    "        #print(\"Post interpolate and conv:\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecodingModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, leakiness=1e-2, conv_bias=True, kernel_size=3,\n",
    "        inst_norm_affine=True, res=True, lrelu_inplace=True):\n",
    "        \"\"\"[The Decoding convolution module to learn the information and use later]\n",
    "        \n",
    "        [This function will create the Learning convolutions]\n",
    "        \n",
    "        Arguments:\n",
    "            input_channels {[int]} -- [the input number of channels, in our case\n",
    "                                       the number of channels from downsample]\n",
    "            output_channels {[int]} -- [the output number of channels, will det-\n",
    "                                        -ermine the upcoming channels]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            kernel_size {number} -- [size of filter] (default: {3})\n",
    "            leakiness {number} -- [the negative leakiness] (default: {1e-2})\n",
    "            conv_bias {bool} -- [to use the bias in filters] (default: {True})\n",
    "            inst_norm_affine {bool} -- [affine use in norm] (default: {True})\n",
    "            res {bool} -- [to use residual connections] (default: {False})\n",
    "            lrelu_inplace {bool} -- [To update conv outputs with lrelu outputs] \n",
    "                                    (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.res = res\n",
    "        self.in_0 = nn.InstanceNorm3d(input_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_1 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_2 = nn.InstanceNorm3d(output_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv1 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv2 = nn.Conv3d(output_channels, output_channels, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print(x.shape)\n",
    "        x = F.leaky_relu(self.in_0(x))\n",
    "        x = self.conv0(x)\n",
    "        if self.res == True:\n",
    "            skip = x\n",
    "        x = F.leaky_relu(self.in_1(x))\n",
    "        x = F.leaky_relu(self.in_2(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        if self.res == True:\n",
    "            x = x + skip\n",
    "        return x\n",
    "\n",
    "class out_conv(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, leakiness=1e-2, kernel_size=3,\n",
    "        conv_bias=True, inst_norm_affine=True, res=True, lrelu_inplace=True):\n",
    "        \"\"\"[The Out convolution module to learn the information and use later]\n",
    "        \n",
    "        [This function will create the Learning convolutions]\n",
    "        \n",
    "        Arguments:\n",
    "            input_channels {[int]} -- [the input number of channels, in our case\n",
    "                                       the number of channels from downsample]\n",
    "            output_channels {[int]} -- [the output number of channels, will det-\n",
    "                                        -ermine the upcoming channels]\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            kernel_size {number} -- [size of filter] (default: {3})\n",
    "            leakiness {number} -- [the negative leakiness] (default: {1e-2})\n",
    "            conv_bias {bool} -- [to use the bias in filters] (default: {True})\n",
    "            inst_norm_affine {bool} -- [affine use in norm] (default: {True})\n",
    "            res {bool} -- [to use residual connections] (default: {False})\n",
    "            lrelu_inplace {bool} -- [To update conv outputs with lrelu outputs] \n",
    "                                    (default: {True})\n",
    "        \"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.lrelu_inplace = lrelu_inplace\n",
    "        self.inst_norm_affine = inst_norm_affine\n",
    "        self.conv_bias = conv_bias\n",
    "        self.leakiness = leakiness\n",
    "        self.res = res\n",
    "        self.in_0 = nn.InstanceNorm3d(input_channels, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_1 = nn.InstanceNorm3d(input_channels//2, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_2 = nn.InstanceNorm3d(input_channels//2, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.in_3 = nn.InstanceNorm3d(input_channels//2, \n",
    "                                      affine=self.inst_norm_affine,\n",
    "                                      track_running_stats=True)\n",
    "        self.conv0 = nn.Conv3d(input_channels, input_channels//2, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv1 = nn.Conv3d(input_channels//2, input_channels//2, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv2 = nn.Conv3d(input_channels//2, input_channels//2, kernel_size=3,\n",
    "                               stride=1, padding=(kernel_size - 1) // 2, \n",
    "                               bias=self.conv_bias)\n",
    "        self.conv3 = nn.Conv3d(input_channels//2, output_channels, kernel_size=1,\n",
    "                               stride=1, padding=0, \n",
    "                               bias=self.conv_bias)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print(x.shape)\n",
    "        x = F.leaky_relu(self.in_0(x))\n",
    "        x = self.conv0(x)\n",
    "        if self.res == True:\n",
    "            skip = x\n",
    "        x = F.leaky_relu(self.in_1(x))\n",
    "        x = F.leaky_relu(self.in_2(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        if self.res == True:\n",
    "            x = x + skip\n",
    "        x = F.leaky_relu(self.in_3(x))\n",
    "        x = F.sigmoid(self.conv3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unet_light(nn.Module):\n",
    "\tdef __init__(self, n_channels, n_classes):\n",
    "\t\tsuper(unet_light, self).__init__()\n",
    "\t\tself.n_channels = n_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.ins = in_conv(self.n_channels, 8)\n",
    "\t\tself.ds_0 = DownsamplingModule(8, 16)\n",
    "\t\tself.en_1 = EncodingModule(16, 16)\n",
    "\t\tself.ds_1 = DownsamplingModule(16, 32)\n",
    "\t\tself.en_2 = EncodingModule(32, 32)\n",
    "\t\tself.ds_2 = DownsamplingModule(32, 64)\n",
    "\t\tself.en_3 = EncodingModule(64, 64)\n",
    "\t\tself.ds_3 = DownsamplingModule(64, 128)\n",
    "\t\tself.en_4 = EncodingModule(128, 128)\n",
    "\t\tself.us_3 = UpsamplingModule(128, 64)\n",
    "\t\tself.de_3 = DecodingModule(128, 64)\n",
    "\t\tself.us_2 = UpsamplingModule(64, 32)\n",
    "\t\tself.de_2 = DecodingModule(64, 32)\n",
    "\t\tself.us_1 = UpsamplingModule(32, 16)\n",
    "\t\tself.de_1 = DecodingModule(32, 16)\n",
    "\t\tself.us_0 = UpsamplingModule(16, 8)\n",
    "\t\tself.out = out_conv(16, self.n_classes-1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.ins(x)\n",
    "\t\tx2 = self.ds_0(x1)\n",
    "\t\tx2 = self.en_1(x2)\n",
    "\t\tx3 = self.ds_1(x2)\n",
    "\t\tx3 = self.en_2(x3)\n",
    "\t\tx4 = self.ds_2(x3)\n",
    "\t\tx4 = self.en_3(x4)\n",
    "\t\tx5 = self.ds_3(x4)\n",
    "\t\tx5 = self.en_4(x5)\n",
    "\n",
    "\t\tx = self.us_3(x5)\n",
    "\t\tx = self.de_3(x, x4)\n",
    "\t\tx = self.us_2(x)\n",
    "\t\tx = self.de_2(x, x3)\n",
    "\t\tx = self.us_1(x)\n",
    "\t\tx = self.de_1(x, x2)\n",
    "\t\tx = self.us_0(x)\n",
    "\t\tx = self.out(x, x1)\n",
    "\t\treturn x\n",
    "\n",
    "class unet_crisp(nn.Module):\n",
    "\tdef __init__(self, n_channels, n_classes):\n",
    "\t\tsuper(unet_crisp, self).__init__()\n",
    "\t\tself.n_channels = n_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.ins = in_conv(self.n_channels, 4)\n",
    "\t\tself.ds_0 = DownsamplingModule(4, 8)\n",
    "\t\tself.en_1 = EncodingModule(8, 8)\n",
    "\t\tself.ds_1 = DownsamplingModule(8, 16)\n",
    "\t\tself.en_2 = EncodingModule(16, 16)\n",
    "\t\tself.ds_2 = DownsamplingModule(16, 32)\n",
    "\t\tself.en_3 = EncodingModule(32, 32)\n",
    "\t\tself.ds_3 = DownsamplingModule(32, 64)\n",
    "\t\tself.en_4 = EncodingModule(64, 64)\n",
    "\t\tself.us_3 = UpsamplingModule(64, 32)\n",
    "\t\tself.de_3 = DecodingModule(64, 32)\n",
    "\t\tself.us_2 = UpsamplingModule(32, 16)\n",
    "\t\tself.de_2 = DecodingModule(32, 16)\n",
    "\t\tself.us_1 = UpsamplingModule(16, 8)\n",
    "\t\tself.de_1 = DecodingModule(16, 8)\n",
    "\t\tself.us_0 = UpsamplingModule(8, 4)\n",
    "\t\tself.out = out_conv(8, self.n_classes-1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.ins(x)\n",
    "\t\tx2 = self.ds_0(x1)\n",
    "\t\tx2 = self.en_1(x2)\n",
    "\t\tx3 = self.ds_1(x2)\n",
    "\t\tx3 = self.en_2(x3)\n",
    "\t\tx4 = self.ds_2(x3)\n",
    "\t\tx4 = self.en_3(x4)\n",
    "\t\tx5 = self.ds_3(x4)\n",
    "\t\tx5 = self.en_4(x5)\n",
    "\n",
    "\t\tx = self.us_3(x5)\n",
    "\t\tx = self.de_3(x, x4)\n",
    "\t\tx = self.us_2(x)\n",
    "\t\tx = self.de_2(x, x3)\n",
    "\t\tx = self.us_1(x)\n",
    "\t\tx = self.de_1(x, x2)\n",
    "\t\tx = self.us_0(x)\n",
    "\t\tx = self.out(x, x1)\n",
    "\t\treturn x\n",
    "    \n",
    "class unet(nn.Module):\n",
    "\tdef __init__(self, n_channels, n_classes):\n",
    "\t\tsuper(unet, self).__init__()\n",
    "\t\tself.n_channels = n_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.ins = in_conv(self.n_channels, 16)\n",
    "\t\tself.ds_0 = DownsamplingModule(16, 32)\n",
    "\t\tself.en_1 = EncodingModule(32, 32)\n",
    "\t\tself.ds_1 = DownsamplingModule(32, 64)\n",
    "\t\tself.en_2 = EncodingModule(64, 64)\n",
    "\t\tself.ds_2 = DownsamplingModule(64, 128)\n",
    "\t\tself.en_3 = EncodingModule(128, 128)\n",
    "\t\tself.ds_3 = DownsamplingModule(128, 256)\n",
    "\t\tself.en_4 = EncodingModule(256, 256)\n",
    "\t\tself.us_3 = UpsamplingModule(256, 128)\n",
    "\t\tself.de_3 = DecodingModule(256, 128)\n",
    "\t\tself.us_2 = UpsamplingModule(128, 64)\n",
    "\t\tself.de_2 = DecodingModule(128, 64)\n",
    "\t\tself.us_1 = UpsamplingModule(64, 32)\n",
    "\t\tself.de_1 = DecodingModule(64, 32)\n",
    "\t\tself.us_0 = UpsamplingModule(32, 16)\n",
    "\t\tself.out = out_conv(32, self.n_classes-1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.ins(x)\n",
    "\t\tx2 = self.ds_0(x1)\n",
    "\t\tx2 = self.en_1(x2)\n",
    "\t\tx3 = self.ds_1(x2)\n",
    "\t\tx3 = self.en_2(x3)\n",
    "\t\tx4 = self.ds_2(x3)\n",
    "\t\tx4 = self.en_3(x4)\n",
    "\t\tx5 = self.ds_3(x4)\n",
    "\t\tx5 = self.en_4(x5)\n",
    "\n",
    "\t\tx = self.us_3(x5)\n",
    "\t\tx = self.de_3(x, x4)\n",
    "\t\tx = self.us_2(x)\n",
    "\t\tx = self.de_2(x, x3)\n",
    "\t\tx = self.us_1(x)\n",
    "\t\tx = self.de_1(x, x2)\n",
    "\t\tx = self.us_0(x)\n",
    "\t\tx = self.out(x, x1)\n",
    "\t\treturn x\n",
    "\n",
    "class resunet(nn.Module):\n",
    "\tdef __init__(self, n_channels, n_classes):\n",
    "\t\tsuper(resunet, self).__init__()\n",
    "\t\tself.n_channels = n_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.ins = in_conv(self.n_channels, 16, res=True)\n",
    "\t\tself.ds_0 = DownsamplingModule(16, 32)\n",
    "\t\tself.en_1 = EncodingModule(32, 32, res=True)\n",
    "\t\tself.ds_1 = DownsamplingModule(32, 64)\n",
    "\t\tself.en_2 = EncodingModule(64, 64, res=True)\n",
    "\t\tself.ds_2 = DownsamplingModule(64, 128)\n",
    "\t\tself.en_3 = EncodingModule(128, 128, res=True)\n",
    "\t\tself.ds_3 = DownsamplingModule(128, 256)\n",
    "\t\tself.en_4 = EncodingModule(256, 256, res=True)\n",
    "\t\tself.us_3 = UpsamplingModule(256, 128)\n",
    "\t\tself.de_3 = DecodingModule(256, 128, res=True)\n",
    "\t\tself.us_2 = UpsamplingModule(128, 64)\n",
    "\t\tself.de_2 = DecodingModule(128, 64, res=True)\n",
    "\t\tself.us_1 = UpsamplingModule(64, 32)\n",
    "\t\tself.de_1 = DecodingModule(64, 32, res=True)\n",
    "\t\tself.us_0 = UpsamplingModule(32, 16)\n",
    "\t\tself.out = out_conv(32, self.n_classes-1, res=True)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.ins(x)\n",
    "\t\tx2 = self.ds_0(x1)\n",
    "\t\tx2 = self.en_1(x2)\n",
    "\t\tx3 = self.ds_1(x2)\n",
    "\t\tx3 = self.en_2(x3)\n",
    "\t\tx4 = self.ds_2(x3)\n",
    "\t\tx4 = self.en_3(x4)\n",
    "\t\tx5 = self.ds_3(x4)\n",
    "\t\tx5 = self.en_4(x5)\n",
    "\n",
    "\t\tx = self.us_3(x5)\n",
    "\t\tx = self.de_3(x, x4)\n",
    "\t\tx = self.us_2(x)\n",
    "\t\tx = self.de_2(x, x3)\n",
    "\t\tx = self.us_1(x)\n",
    "\t\tx = self.de_1(x, x2)\n",
    "\t\tx = self.us_0(x)\n",
    "\t\tx = self.out(x, x1)\n",
    "\t\treturn x\n",
    "\n",
    "class fcn(nn.Module):\n",
    "\tdef __init__(self, n_channels, n_classes):\n",
    "\t\tsuper(fcn, self).__init__()\n",
    "\t\tself.n_channels = n_channels\n",
    "\t\tself.n_classes = n_classes\n",
    "\t\tself.ins = in_conv(self.n_channels, 16)\n",
    "\t\tself.ds_0 = DownsamplingModule(16, 32)\n",
    "\t\tself.en_1 = EncodingModule(32, 32)\n",
    "\t\tself.ds_1 = DownsamplingModule(32, 64)\n",
    "\t\tself.en_2 = EncodingModule(64, 64)\n",
    "\t\tself.ds_2 = DownsamplingModule(64, 128)\n",
    "\t\tself.en_3 = EncodingModule(128, 128)\n",
    "\t\tself.ds_3 = DownsamplingModule(128, 256)\n",
    "\t\tself.en_4 = EncodingModule(256, 256)\n",
    "\t\tself.us_4 = FCNUpsamplingModule(256, 1, scale_factor = 5)\n",
    "\t\tself.us_3 = FCNUpsamplingModule(128, 1, scale_factor = 4)\n",
    "\t\tself.us_2 = FCNUpsamplingModule(64, 1, scale_factor = 3)\n",
    "\t\tself.us_1 = FCNUpsamplingModule(32, 1, scale_factor = 2)\n",
    "\t\tself.us_0 = FCNUpsamplingModule(16, 1, scale_factor = 1)\n",
    "\t\tself.conv_0 = nn.Conv3d(in_channels=5, out_channels=self.n_classes-1, kernel_size=1, stride=1, padding=0, bias = True)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx1 = self.ins(x)\n",
    "\t\tx2 = self.ds_0(x1)\n",
    "\t\tx2 = self.en_1(x2)\n",
    "\t\tx3 = self.ds_1(x2)\n",
    "\t\tx3 = self.en_2(x3)\n",
    "\t\tx4 = self.ds_2(x3)\n",
    "\t\tx4 = self.en_3(x4)\n",
    "\t\tx5 = self.ds_3(x4)\n",
    "\t\tx5 = self.en_4(x5)\n",
    "\n",
    "\t\tu5 = self.us_4(x5)\n",
    "\t\tu4 = self.us_3(x4)\n",
    "\t\tu3 = self.us_2(x3)\n",
    "\t\tu2 = self.us_1(x2)\n",
    "\t\tu1 = self.us_0(x1)\n",
    "\t\tx = torch.cat([u5, u4, u3, u2, u1], dim=1)\n",
    "\t\tx = self.conv_0(x)\n",
    "\t\treturn F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = unet(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from tfedlrn.datasets import load_dataset\n",
    "\n",
    "def reshape_for_3d(x):\n",
    "    v = x.reshape(x.shape[0] // 155, 1, 155, 128, 128)\n",
    "    return v[:, :, 14:142, :, :]\n",
    "\n",
    "def create_loader(X, y, **kwargs):\n",
    "    tX = torch.stack([torch.Tensor(i) for i in X])\n",
    "    ty = torch.stack([torch.Tensor(i) for i in y])\n",
    "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(tX, ty), **kwargs)\n",
    "\n",
    "def init_data_pipeline(batch_size=1):\n",
    "    # load all the institutions\n",
    "    data_by_institution = [load_dataset('BraTS17_institution',\n",
    "                                        channels_first=True,\n",
    "                                        institution=i) for i in range(10)]\n",
    "    data_by_type = zip(*data_by_institution)\n",
    "    data_by_type = [np.concatenate(d) for d in data_by_type]\n",
    "    X_train, y_train, X_val, y_val = [reshape_for_3d(d) for d in data_by_type]\n",
    "    return (create_loader(X_train, y_train, batch_size=batch_size, shuffle=True), \n",
    "            create_loader(X_val, y_val, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "train_loader, val_loader = init_data_pipeline(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# import torch.utils.data\n",
    "# device = torch.device('cpu')\n",
    "# data = torch.Tensor(np.random.random(size=155*128*128).reshape(1, 1, 128, 128, 128))\n",
    "# data = data.to(device)\n",
    "# net = net.to(device)\n",
    "# output = net(data)\n",
    "# make_dot(output, params=dict(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(pred, target, smoothing=1.0, dim=(1,2,3,4)):    \n",
    "    intersection = (pred * target).sum(dim=dim)\n",
    "    union = (pred + target).sum(dim=dim)\n",
    "    \n",
    "    return ((2 * intersection + smoothing) / (union + smoothing)).mean()\n",
    "\n",
    "\n",
    "def dice_coef_loss(pred, target, smoothing=1.0, dim=(1,2,3,4)):    \n",
    "    intersection = (pred * target).sum(dim=dim)\n",
    "    union = (pred + target).sum(dim=dim)\n",
    "    \n",
    "    term1 = -torch.log(2 * intersection + smoothing)\n",
    "    term2 = torch.log(union + smoothing)\n",
    "    \n",
    "    return term1.mean() + term2.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def init_optimizer(net, optimizer='RMSprop', lr=1e-4, momentum=0.9):\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr, momentum=momentum)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return optimizer\n",
    "\n",
    "def train_epoch(net, train_loader, device, optimizer):\n",
    "    # set to \"training\" mode\n",
    "    net.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = dice_coef_loss(output, target, smoothing=4.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def validate(net, val_loader, device):\n",
    "    net.eval()\n",
    "    dice = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            dice += dice_coef(output, target).cpu().numpy() * samples\n",
    "    return dice / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = init_optimizer(net, optimizer='Adam', lr=1e-4)\n",
    "optimizer = init_optimizer(net, optimizer='RMSprop', lr=1e-5, momentum=0.9)\n",
    "# optimizer = init_optimizer(net, optimizer='SGD', lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msheller/.local/share/virtualenvs/tfedlrn-CPfKUQY7/lib/python3.5/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch 0 : 4.5720387\n",
      "dice for epoch 0 : 0.10173853304651048\n",
      "loss for epoch 1 : 4.463547\n",
      "dice for epoch 1 : 0.10610134568479326\n",
      "loss for epoch 2 : 4.399174\n",
      "dice for epoch 2 : 0.10362325575616625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dc1a610a0330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss for epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dice for epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e5b40e8f5d73>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, train_loader, device, optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_coef_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tfedlrn-CPfKUQY7/lib/python3.5/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'centered'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(16):\n",
    "    print('loss for epoch', e, ':', train_epoch(net, train_loader, device, optimizer))\n",
    "    print('dice for epoch', e, ':', validate(net, val_loader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    print(dice_coef(target, target))\n",
    "#     optimizer.zero_grad()\n",
    "#     output = net(data)\n",
    "#     loss = dice_coef_loss(output, target, smoothing=4.0)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     losses.append(loss.detach().cpu().numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tumor pixels are a bell shape, starting around 30 and ending around 138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfedlrn-CPfKUQY7",
   "language": "python",
   "name": "tfedlrn-cpfkuqy7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
