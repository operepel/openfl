{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated Keras MNIST Tutorial\n",
    "## Using low-level Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install tensorflow==2.3.1\n",
    "!pip install scikit-image\n",
    "!pip install cloudpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A keras model\n",
    "\"\"\"\n",
    "feature_shape = 784\n",
    "classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "    \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-metallic",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-remainder",
   "metadata": {},
   "source": [
    "We ask user to keep all the test data in `data/` folder under the workspace as it will not be sent to collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "VALID_PERCENT = 0.3\n",
    "\n",
    "split_on = int((1 - VALID_PERCENT) * len(X_train))\n",
    "\n",
    "train_images = X_train[0:split_on,:,:]\n",
    "train_labels = to_categorical(y_train)[0:split_on,:]\n",
    "\n",
    "valid_images = X_train[split_on:,:,:]\n",
    "valid_labels = to_categorical(y_train)[split_on:,:]\n",
    "\n",
    "test_images = X_test\n",
    "test_labels = to_categorical(y_test)\n",
    "\n",
    "def preprocess(images):\n",
    "    #Normalize\n",
    "    images = (images / 255) - 0.5\n",
    "    #Flatten\n",
    "    images = images.reshape((-1, 784))\n",
    "    return images\n",
    "\n",
    "# Preprocess the images.\n",
    "train_images = preprocess(train_images)\n",
    "valid_images = preprocess(valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Describing FL experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, framework_plugin=framework_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-texas",
   "metadata": {},
   "source": [
    "We extract User dataset class implementation.\n",
    "Is it convinient?\n",
    "What if the dataset is not a class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedDataset(DataInterface):\n",
    "    \"\"\"\n",
    "    The set of initialization parameters for the FedDataset can be customized.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_images, train_labels, valid_images, valid_labels, **kwargs):\n",
    "        self.X_train = train_images\n",
    "        self.y_train = train_labels\n",
    "        self.X_valid = valid_images\n",
    "        self.y_valid = valid_labels\n",
    "        self.batch_size = 32\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def _delayed_init(self, data_path='1,1'):\n",
    "        # With the next command the local dataset will be loaded on the collaborator node\n",
    "        # For this example we have the same dataset on the same path, and we will shard it\n",
    "        # So we use `data_path` information for this purpose.\n",
    "        self.rank, self.world_size = [int(part) for part in data_path.split(',')]\n",
    "        \n",
    "        # Do the actual sharding\n",
    "        self._do_sharding( self.rank, self.world_size)\n",
    "        \n",
    "    def _do_sharding(self, rank, world_size):\n",
    "        # This method relies on the dataset's implementation\n",
    "        # i.e. coupled in a bad way\n",
    "        self.X_train = self.X_train[ rank-1 :: world_size ]\n",
    "        self.y_train = self.y_train[ rank-1 :: world_size ]\n",
    "        self.X_valid = self.X_valid[ rank-1 :: world_size ]\n",
    "        self.y_valid = self.y_valid[ rank-1 :: world_size ]\n",
    "        \n",
    "\n",
    "    def get_train_loader(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Get training data loader.\n",
    "        Returns\n",
    "        -------\n",
    "        loader object\n",
    "        \"\"\"\n",
    "        return self.X_train\n",
    "\n",
    "    def get_valid_loader(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Get validation data loader.\n",
    "        Returns:\n",
    "            loader object\n",
    "        \"\"\"\n",
    "        return self.X_valid\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of training samples.\n",
    "        Returns:\n",
    "            int: number of training samples\n",
    "        \"\"\"\n",
    "        return self.X_train.shape[0]\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of validation samples.\n",
    "        Returns:\n",
    "            int: number of validation samples\n",
    "        \"\"\"\n",
    "        return self.X_valid.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _batch_generator(X, y, idxs, batch_size, num_batches):\n",
    "        \"\"\"\n",
    "        Generate batch of data.\n",
    "        Args:\n",
    "            X: input data\n",
    "            y: label data\n",
    "            idxs: The index of the dataset\n",
    "            batch_size: The batch size for the data loader\n",
    "            num_batches: The number of batches\n",
    "        Yields:\n",
    "            tuple: input data, label data\n",
    "        \"\"\"\n",
    "        for i in range(num_batches):\n",
    "            a = i * batch_size\n",
    "            b = a + batch_size\n",
    "            yield X[idxs[a:b]], y[idxs[a:b]]\n",
    "\n",
    "    def _get_batch_generator(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return the dataset generator.\n",
    "        Args:\n",
    "            X: input data\n",
    "            y: label data\n",
    "            batch_size: The batch size for the data loader\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        # shuffle data indices\n",
    "        idxs = np.random.permutation(np.arange(X.shape[0]))\n",
    "\n",
    "        # compute the number of batches\n",
    "        num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "\n",
    "        # build the generator and return it\n",
    "        return self._batch_generator(X, y, idxs, batch_size, num_batches)\n",
    "    \n",
    "fed_dataset = FedDataset(train_images=train_images,\n",
    "                         train_labels=train_labels,\n",
    "                         valid_images=valid_images,\n",
    "                         valid_labels=valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-kansas",
   "metadata": {},
   "source": [
    "### Register tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "# We do not actually need to register additional kwargs, Just serialize them\n",
    "@TI.add_kwargs(**{'batch_size': 32})\n",
    "@TI.register_fl_task(model='model', data_loader='train_loader')\n",
    "@TI.send_model()\n",
    "def train(model, train_loader, batch_size=1):\n",
    "    \n",
    "    history = model.fit(train_loader.X_train,\n",
    "                        train_loader.y_train,\n",
    "                        batch_size=train_loader.batch_size,\n",
    "                                 epochs=1,\n",
    "                                 verbose=0, )\n",
    "    \n",
    "    return {str(metric_name): np.mean([history.history[metric_name]]) for metric_name in model.metrics_names}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='val_loader')     \n",
    "def validate(model, val_loader):\n",
    "    \n",
    "    vals = model.evaluate(\n",
    "            val_loader.X_valid,\n",
    "            val_loader.y_valid,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "    )\n",
    "    \n",
    "    return {'accuracy': np.mean(vals[1]),}\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "# will determine fqdn by itself\n",
    "federation = Federation(central_node_fqdn='localhost', disable_tls=True)\n",
    "col_data_paths = {'one': '1,2',\n",
    "                'two': '2,2'}\n",
    "federation.register_collaborators(col_data_paths=col_data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "fl_experiment = FLExperiment(federation=federation, serializer_plugin='openfl.plugins.interface_serializer.cloudpickle_serializer.Cloudpickle_Serializer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "fl_experiment.start_experiment(model_provider=MI, task_keeper=TI, data_loader=fed_dataset, rounds_to_train=5, \\\n",
    "                              opt_treatment='CONTINUE_GLOBAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.plan.config['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-white",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fl_experiment.plan.config['assigner']['settings']['task_groups'][0]['tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.plan.Build(fl_experiment.plan.config['api_layer']['required_plugin_components']['serializer_plugin'], {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-valentine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl_experiment.plan.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml.safe_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-authentication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-electric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('./tasks.pkl', 'wb') as f:\n",
    "    dill.dump(TI, f,recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.arange(0,10)\n",
    "test_task(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-socket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('./model.pkl', 'wb') as f:\n",
    "    dill.dump(MI, f, recurse=True)\n",
    "# Pickling class    \n",
    "# with open('./model_cls.pkl', 'wb') as f:\n",
    "#     dill.dump(UNet, f, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(1,3,128,128)\n",
    "model_unet.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-sessions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('./dataloader.pkl', 'wb') as f:\n",
    "    dill.dump(fed_dataset, f, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data.yaml', 'w') as f:\n",
    "    for col_name, data_path in {'one': '1,2',\n",
    "            'two': '2,2'}.items():\n",
    "        f.write(f'{col_name},{data_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_contract = dict()\n",
    "task_contract['optimizer'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = True if task_contract['optimizer'] is not None else False\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
